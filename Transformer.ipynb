{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyarrow\n",
        "!pip install pandas scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96DdsOtDnGWU",
        "outputId": "2bec23c2-eb19-4476-c5f3-cb2fce9aaef1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (18.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore',category=UserWarning)"
      ],
      "metadata": {
        "id": "AgGUjfz52Ms2"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn as nn\n",
        "import math"
      ],
      "metadata": {
        "id": "1_xfuW8bnMgs"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train_df = pd.read_parquet('/content/drive/MyDrive/train.parquet')\n",
        "test_df = pd.read_parquet('/content/drive/MyDrive/test.parquet')\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, stratify=train_df['label'], random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2SBLbKYV8CF",
        "outputId": "dc663f2e-31c1-4eab-d0f0-78d502a9ff02"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(texts, min_freq=2):\n",
        "    counter = Counter()\n",
        "    for text in texts:\n",
        "        tokens = text.lower().split()\n",
        "        counter.update(tokens)\n",
        "    vocab = {\"<unk>\": 0, \"<pad>\": 1}\n",
        "    for word, freq in counter.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[word] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab(train_df['text'])"
      ],
      "metadata": {
        "id": "cRqW5KPtnJvD"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_indices(text, vocab):\n",
        "    return [vocab.get(token, vocab[\"<unk>\"]) for token in text.lower().split()]\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, df, vocab):\n",
        "        self.texts = df['text'].tolist()\n",
        "        self.labels = df['label'].tolist()\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = torch.tensor(text_to_indices(self.texts[idx], self.vocab), dtype=torch.long)\n",
        "        return tokens, self.labels[idx]\n",
        "\n",
        "def collate_batch(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    texts = pad_sequence(texts, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
        "    return texts, torch.tensor(labels)"
      ],
      "metadata": {
        "id": "BYA7xSKpncpL"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(TextDataset(train_df, vocab), batch_size=64, shuffle=True, collate_fn=collate_batch)\n",
        "val_loader = DataLoader(TextDataset(val_df, vocab), batch_size=64, shuffle=False, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(TextDataset(test_df, vocab), batch_size=64, shuffle=False, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "Hhj-pFd8nkWt"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
        "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, output_dim, pad_idx,max_len=512):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "        self.pos_encoder = PositionalEncoding(embed_dim,max_len=max_len)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim,batch_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(embed_dim, output_dim)\n",
        "        self.pad_idx=pad_idx\n",
        "\n",
        "    def forward(self, x):\n",
        "        mask = (x == vocab[\"<pad>\"])\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer_encoder(x, src_key_padding_mask=mask).mean(dim=1)\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "BYm_RRTInnz7"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = TransformerClassifier(\n",
        "    vocab_size=len(vocab),\n",
        "    embed_dim=128,\n",
        "    num_heads=4,\n",
        "    hidden_dim=256,\n",
        "    num_layers=2,\n",
        "    output_dim=4,\n",
        "    pad_idx=vocab[\"<pad>\"]\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "def train_epoch(model, loader):\n",
        "    model.train()\n",
        "    total_loss, total_correct = 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * len(y)\n",
        "        total_correct += (output.argmax(1) == y).sum().item()\n",
        "    return total_loss / len(loader.dataset), total_correct / len(loader.dataset)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total_loss, total_correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            output = model(x)\n",
        "            loss = criterion(output, y)\n",
        "            total_loss += loss.item() * len(y)\n",
        "            total_correct += (output.argmax(1) == y).sum().item()\n",
        "    return total_loss / len(loader.dataset), total_correct / len(loader.dataset)"
      ],
      "metadata": {
        "id": "Sma1POUvnxAW"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 6):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader)\n",
        "    val_loss, val_acc = evaluate(model, val_loader)\n",
        "    print(f\"Epoch {epoch}: Train loss {train_loss:.4f}, acc {train_acc:.4f} | Val loss {val_loss:.4f}, acc {val_acc:.4f}\")\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_loader)\n",
        "print(f\"Final Test loss: {test_loss:.4f}, accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqQDEc0pn0qj",
        "outputId": "95ccd310-00dd-45cf-a4b2-4a30f37c1abf"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train loss 0.5881, acc 0.7747 | Val loss 0.5388, acc 0.8627\n",
            "Epoch 2: Train loss 0.3423, acc 0.8788 | Val loss 0.4559, acc 0.8876\n",
            "Epoch 3: Train loss 0.2684, acc 0.9065 | Val loss 0.3940, acc 0.8984\n",
            "Epoch 4: Train loss 0.2193, acc 0.9238 | Val loss 0.3605, acc 0.8982\n",
            "Epoch 5: Train loss 0.1805, acc 0.9373 | Val loss 0.3519, acc 0.8971\n",
            "Final Test loss: 0.3625, accuracy: 0.8978\n"
          ]
        }
      ]
    }
  ]
}